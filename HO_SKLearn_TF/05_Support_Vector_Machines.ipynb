{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Code-Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "Well suited for complex small and medium datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM\n",
    "* Large Margin Classification\n",
    "* SVMs are sensitive to feature scales.  \n",
    "**Soft Margin Classification**\n",
    "* Find a good balance between keeping the street as large as possible and limiting the margin violations.\n",
    "* Can be controlled in sklearn with the C hyperparameter - Smaller C - Wider Street - More margin violations - and vice versa.\n",
    "* If SVM is overfitting - regularization by reducing C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "    ])\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** SVM Classifiers do not output probabilities.  \n",
    "**NOTE:** Linear Kernel is slower for larger datasets. \n",
    "**NOTE:** SGDClassifier with loss hinge and alpha=1/(mC) for training a linear SVM. It does not converge faster, but helps training large datasets.  \n",
    "**NOTE:** The LinearSVC class regularizes the bias term, so you should center\n",
    "the training set first by subtracting its mean. Standard Scaler does this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Non-Linear separable features - move to polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline([\n",
    "(\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Low Polynomial Degree - cannot deal with complex datasets - High - model too slow.\n",
    "**NOTE**: Kernel Trick - Same results as a lot of polynomials without adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: coef0 controls how much the polynomial is influenced by high-degree polynomials vs low-degree ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding Similarity Features\n",
    "**NOTE**: Adding a feature, computed with a similarity function to measure hom much each instance resembles a particular landmark f.e. Gaussian RBF.\n",
    "**NOTE**: Landmark at each instance of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian RBF Kernel\n",
    "Just like polynomials - GRBF Kernels can be used to be added to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** parameter gamma controls the similarity bell curve - either narrow or wide (small gamma - wide and vice-versa)  \n",
    "**NOTE** RULE OF THUMB - Try First LinearSVC and then if dataset not too large - Gaussian RBF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Linear SVC O(mxn) SVC -O(m^2xn) - slow when dealing with > 100000. Scales well with Sparse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Regression\n",
    "* Control the street with epsilon - high /epsilon - large street\n",
    "* /epsilon-sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Optimization algos wwrok much better on differentiable functions.\n",
    "**NOTE**: Convex Optimization with linear contraints - Quadratic Programing - Solvers Exist.  \n",
    "**NOTE**: The Dual Problem - Lower bound to the solution of the primal problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: A kernel is a function capable of computing the dot product of f(a)T f(b) based only on the original a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mercer's Theorem**  \n",
    "According to Mercer’s theorem, if a function K(a, b) respects a few mathematical \n",
    "conditions called Mercer’s conditions (K must be continuous, symmetric in its arguments\n",
    "so K(a, b) = K(b, a), etc.), then there exists a function φ that maps a and b into\n",
    "another space (possibly with much higher dimensions) such that K(a, b) = φ(a) T φ(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hinge Loss**\n",
    "The function max(0, 1 – t) is called the hinge loss function. It is\n",
    "equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not differentiable at t = 1, but just like for Lasso Regression you can still use Gradient Descent using any subderivative at t = 1 (i.e., any value between –1 and 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the fundamental idea behind Support Vector Machines?\n",
    "The idea is to make a decision boundary between classes within a margin of error by creating a plane h = wx + b (if x is our only feature) and having it at 0.\n",
    "2. What is a support vector?\n",
    "The points at which the h is at -1 or 1.\n",
    "3. Why is it important to scale the inputs when using SVMs?\n",
    "Because scale can impact the decision boundary.\n",
    "4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "No for both. SVM Classifier outputs only the predicted class.\n",
    "5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "Yes - the dual form is effective when n > m.\n",
    "6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease γ ( gamma )? What about C ?\n",
    "Increase bot C and gamma.\n",
    "7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "...\n",
    "8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.\n",
    "This could be achieved with 'hinge loss'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
