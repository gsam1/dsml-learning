{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - GRADIENT DESCENT AND LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Course Notebook on Github](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson4-mnist_sgd.ipynb)\n",
    "\n",
    "### Notes\n",
    "My goal is not to copy what it is taught in the course and in the notebook. Its just various notes and trying things for my own.\n",
    "Extrapolation in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Personal Notes\n",
    "1. RF and DT are limited to KNN.\n",
    "2. NN are good for unstructured and spatial data.\n",
    "3. Different nomenclature clash in ML. (CV rows and columns)\n",
    "4. Is it important to normalize the variables in RF? No - we care relationships between independent variables, regardless of scale. All that matters is the order. Also immune to outliers. \n",
    "5. For DL - normalize data!\n",
    "6. Always check the descriptives of train/test/validation sets.\n",
    "7. Normalize by channel.\n",
    "8. Different normalization coeff. by features.\n",
    "9. Universal Approximation Theorem.\n",
    "10. What does it need to be a derivative.\n",
    "11. Cost - lower if model is better.\n",
    "11. Negative Loss Likelihood = cross entropy\n",
    "12. One-hot encode the independent variable.\n",
    "13. Predict the probability of the outcome.\n",
    "14. argmax - critical - which is the largest element and return its index.\n",
    "15. NN = linear layer followed by activation function and so on.\n",
    "16. Universal Approximation Theorem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(28*28, 10),\n",
    "    nn.LogSoftmax()\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(y, p):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n",
    "        self.l1_b = get_weights(10)         # Layer 1 bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = (x @ self.l1_w) + self.l1_b  # Linear Layer\n",
    "        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 Pytorch",
   "language": "python",
   "name": "torchpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
